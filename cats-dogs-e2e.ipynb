{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import mxnet as mx\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from time import gmtime, strftime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('train')\n",
    "dogs = [x for x in files if x.find('dog') != -1]\n",
    "cats = [x for x in files if x.find('cat') != -1]\n",
    "targets = [dogs, cats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**Link:**\n",
    "\n",
    "There are 12,500 images of each species contained. Images are larger than thumbnails and are of a variety of breeds. Angles, colors, position are incidental to each photograph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def to_gray(color_img):\n",
    "    gray = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def generate_sift(gray_img):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, desc = sift.detectAndCompute(gray_img, None)\n",
    "    return kp, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting keypoint descriptors from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_desc(file, path='train/'):\n",
    "    \n",
    "    img = cv2.imread(path + file)\n",
    "    img = to_gray(img)\n",
    "    \n",
    "    kp, desc = generate_sift(img)\n",
    "    \n",
    "    num_desc = desc.shape[0]\n",
    "    images = np.full((num_desc,1), file)\n",
    "    \n",
    "    return images, kp, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extract(file_list):\n",
    "    \n",
    "    num_of_files = len(file_list)\n",
    "    i = 1\n",
    "    desc_total = None\n",
    "    kp_total = None\n",
    "    img_total = None\n",
    "    \n",
    "    for file in file_list:\n",
    "        \n",
    "        # Extract all SIFT keypoints and descriptors\n",
    "        images, kp, desc = extract_desc(file)\n",
    "        \n",
    "        if i == 1:\n",
    "            kp_total = kp\n",
    "            desc_total = desc\n",
    "            img_total = images\n",
    "            clear_output(wait=True)\n",
    "            print(i, \"/\", num_of_files, \"completed\")\n",
    "            i = i + 1\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            kp_total = np.append(kp_total, kp)\n",
    "            desc_total = np.vstack((desc_total, desc))\n",
    "            img_total = np.vstack((img_total, images))\n",
    "            clear_output(wait=True)\n",
    "            print(i, \"/\", num_of_files, \"completed\")\n",
    "            i = i + 1\n",
    "    \n",
    "    \n",
    "    return img_total, kp_total, desc_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 500 completed\n"
     ]
    }
   ],
   "source": [
    "dog_img, dog_kp, dog_desc = run_extract(dogs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 500 completed\n"
     ]
    }
   ],
   "source": [
    "cat_img, cat_kp, cat_desc = run_extract(cats[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.7286376953125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_kp[0].angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keypoint object has several attributes, angle being one of them. These could be incorporated in to the model at some point. There may be some benefit to using these attributes to filter descriptors or add bias to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load Data\n",
    "Comment out if not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Cats\n",
    "#np.save('data/20190120-sampledesc-cats', cat_desc)\n",
    "#np.save('data/20190120-img-cats', cat_img)\n",
    "\n",
    "# Save Dogs\n",
    "#np.save('data/20190120-sampledesc-dogs', dog_desc)\n",
    "#np.save('20190120-kmeans-samplelabels-dogs', dog_labels)\n",
    "#np.save('20190120-kmeans-samplecenters-dogs', centers)\n",
    "#np.save('data/20190120-img-dogs', dog_img)\n",
    "\n",
    "# Load Dogs\n",
    "dog_desc = np.load('data/sampledesc-dogs.npy')\n",
    "dog_img = np.load('data/20190120-img-dogs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the cv2 method below, it was far too slow on my personal macbook, engineering laptop, and lab desktop computer. I opted to use AWS Sagemaker for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criteria = ( type, max_iter = 10 , epsilon = 1.0 )\n",
    "#criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "# Set flags (Just to avoid line break in the code)\n",
    "#flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "# Apply KMeans\n",
    "#compactness, dog_lab, dog_cen = cv2.kmeans(dog_desc ,1000,None,criteria,10,flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an S3 instance and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket\n",
    "bucket = 'sagemaker-catsvsdogs-east-1'\n",
    "prefix = 'sagemaker/DEMO-kmeans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_data(filename):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket('cats-vs-dogs-descriptors').download_file(filename, '.desc.npy')\n",
    "    a = np.load('.desc.npy')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'sampledesc-dogs.npy'\n",
    "dog_desc = get_s3_data(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436862, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_desc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to try a standard configuration of AWS SageMaker (K-Means) with 1000 clusters. This will give me feel for how much time is saved by using AWS and also if any cost in incurred. \n",
    "\n",
    "Further efforts could focus around tuning the hyperparameters and building a more robust dictionary (i.e. more images and descriptors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert training data to bytes for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_s3_data(bucket, prefix, channel, X):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, X.astype('float32'))\n",
    "    buf.seek(0)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, channel + '.data')).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_s3_data(bucket, prefix, 'train', dog_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Low-Level SDK method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job kmeans-lowlevel-2019-01-30-08-36-33\n"
     ]
    }
   ],
   "source": [
    "job_name = 'kmeans-lowlevel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training artifacts will be uploaded to: s3://sagemaker-catsvsdogs-east-1/kmeans_lowlevel_example/output\n",
      "InProgress\n",
      "Training job ended with status: Completed\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image = get_image_uri(boto3.Session().region_name, 'kmeans')\n",
    "\n",
    "output_location = 's3://{}/kmeans_lowlevel_example/output'.format(bucket)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "k = '1000'\n",
    "features = '128'\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 2,\n",
    "        \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"k\": k,\n",
    "        \"feature_dim\": features,\n",
    "        \"mini_batch_size\": \"500\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 60\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train.data\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "sagemaker.create_training_job(**create_training_params)\n",
    "\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "\n",
    "try:\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "finally:\n",
    "    status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "    if status == 'Failed':\n",
    "        message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this took about 10 minutes to train and the cost  was $1.38. I tried several times though and was getting an error when it tried to write the file due to region mismatch between the training job and the bucket it was trying to write to. Thus the cost for one training run is likely lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect SageMaker output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kmeans_lowlevel_example/output/kmeans-lowlevel-2019-01-30-08-36-33/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x state_fecf1206-3e2f-4519-bdee-fe419ad22b6e\n",
      "x state_95f27bc9-db6e-4e7c-b5e6-63b649e135f0\n",
      "x model_algo-1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-3bf787a14780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms3_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tar -xvf model.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeans_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_algo-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mx' is not defined"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, path, 'model.tar.gz')\n",
    "!tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = mx.ndarray.load('model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_numpy = kmeans_model[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker output is a list of cluster centers; labels will have to be determined manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local disk\n",
    "#np.save('data/kmeans_numpy', kmeans_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Term Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local\n",
    "kmeans_numpy = np.load('data/kmeans_numpy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "y = np.argmin(cdist(dog_desc, kmeans_numpy, 'euclidean'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_to_visual_words(labels, K=1000):\n",
    "    label_counts = np.zeros(1000)\n",
    "    for k in range(1,K-1):\n",
    "        count = np.where(labels == k)[0].shape[0]\n",
    "        label_counts[k-1] = count\n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_code_book(img_list, labels):\n",
    "    unique_img = np.unique(img_list)\n",
    "    #print(unique_img.shape)\n",
    "    code_book = np.zeros((1000,1000))\n",
    "    i = 0\n",
    "    for img in unique_img:\n",
    "        ix = np.where(np.in1d(img_list, img))[0]\n",
    "        label_hist = desc_to_visual_words(labels[ix])\n",
    "        #print(label_hist)\n",
    "        code_book[i] = label_hist\n",
    "        i = i + 1\n",
    "    \n",
    "    return code_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = build_code_book(dog_img, y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting one image label frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 4. 6. 0. 1. 1. 0. 3. 0. 2. 4. 1. 4. 4. 1. 1. 0. 2. 0. 4. 2. 5. 2. 4.\n",
      " 3. 0. 3. 2. 3. 1. 3. 1. 2. 0. 2. 0. 2. 1. 2. 0. 2. 0. 1. 1. 1. 0. 1. 2.\n",
      " 3. 3. 3. 3. 1. 4. 2. 0. 0. 4. 3. 0. 2. 4. 1. 3. 0. 3. 2. 1. 0. 6. 1. 0.\n",
      " 2. 0. 4. 4. 2. 1. 0. 1. 1. 2. 2. 4. 5. 2. 3. 0. 1. 0. 3. 4. 0. 4. 6. 3.\n",
      " 3. 1. 1. 0. 1. 2. 0. 2. 2. 0. 3. 6. 2. 2. 1. 3. 0. 3. 6. 0. 2. 0. 0. 1.\n",
      " 1. 2. 1. 1. 1. 0. 2. 2. 1. 0. 0. 2. 2. 2. 1. 0. 5. 2. 6. 2. 3. 3. 1. 2.\n",
      " 2. 0. 3. 0. 5. 1. 2. 4. 2. 1. 1. 1. 3. 2. 1. 2. 0. 2. 2. 1. 0. 2. 1. 3.\n",
      " 2. 1. 0. 2. 1. 3. 2. 4. 3. 3. 0. 2. 2. 3. 3. 4. 1. 1. 4. 9. 1. 2. 2. 0.\n",
      " 2. 3. 0. 0. 2. 3. 3. 0. 1. 3. 0. 3. 3. 1. 7. 3. 2. 5. 3. 1. 0. 4. 2. 2.\n",
      " 0. 3. 1. 1. 2. 2. 1. 5. 1. 2. 1. 1. 1. 1. 1. 1. 2. 0. 0. 1. 4. 0. 2. 1.\n",
      " 1. 4. 4. 4. 2. 1. 1. 4. 2. 1. 1. 0. 4. 1. 0. 1. 2. 1. 6. 0. 0. 1. 0. 5.\n",
      " 2. 2. 0. 3. 3. 1. 3. 4. 1. 3. 2. 2. 2. 2. 2. 0. 1. 0. 1. 2. 2. 3. 0. 5.\n",
      " 3. 2. 0. 4. 2. 2. 1. 1. 0. 1. 2. 4. 5. 0. 0. 2. 3. 3. 4. 3. 1. 1. 2. 0.\n",
      " 1. 3. 0. 0. 0. 0. 2. 3. 0. 0. 3. 1. 4. 2. 2. 2. 3. 1. 0. 1. 7. 2. 1. 1.\n",
      " 2. 3. 2. 1. 0. 1. 2. 2. 2. 3. 0. 0. 1. 0. 1. 3. 5. 4. 3. 4. 3. 0. 3. 4.\n",
      " 0. 1. 1. 2. 4. 1. 0. 0. 0. 4. 5. 0. 2. 1. 2. 0. 0. 2. 0. 2. 3. 3. 4. 1.\n",
      " 3. 2. 5. 1. 2. 0. 0. 3. 0. 1. 2. 4. 0. 1. 4. 0. 3. 3. 2. 5. 1. 0. 2. 1.\n",
      " 2. 0. 2. 3. 1. 1. 2. 2. 1. 4. 2. 1. 0. 6. 0. 3. 2. 1. 0. 3. 3. 1. 2. 5.\n",
      " 0. 1. 0. 1. 2. 1. 3. 2. 1. 2. 1. 0. 0. 2. 0. 1. 0. 0. 2. 1. 7. 0. 3. 4.\n",
      " 0. 1. 1. 1. 1. 2. 1. 0. 2. 0. 3. 1. 0. 0. 2. 0. 1. 0. 3. 2. 2. 1. 1. 0.\n",
      " 0. 6. 1. 1. 0. 1. 0. 5. 2. 0. 2. 6. 0. 2. 3. 3. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 2. 2. 3. 0. 2. 1. 2. 0. 2. 0. 0. 1. 3. 2. 4. 1. 1. 1. 1. 0. 3. 3. 0. 0.\n",
      " 2. 4. 1. 8. 0. 1. 2. 2. 1. 1. 0. 1. 4. 1. 3. 3. 1. 0. 3. 3. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 8. 3. 0. 5. 0. 2. 1. 2. 1. 2. 0. 0. 1. 4. 4. 0. 4. 0.\n",
      " 1. 2. 0. 0. 1. 3. 1. 1. 2. 2. 3. 1. 0. 2. 2. 2. 2. 2. 0. 0. 4. 1. 0. 1.\n",
      " 0. 2. 0. 1. 0. 2. 1. 0. 0. 2. 0. 2. 1. 6. 2. 1. 0. 5. 0. 1. 0. 1. 5. 1.\n",
      " 4. 2. 4. 0. 2. 3. 4. 0. 1. 1. 1. 1. 4. 0. 0. 1. 1. 1. 0. 4. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 3. 1. 1. 1. 3. 3. 2. 2. 2. 3. 1. 0. 0. 0. 0. 3. 1. 0. 0.\n",
      " 0. 0. 2. 1. 1. 1. 0. 0. 0. 0. 1. 2. 1. 3. 3. 0. 3. 0. 1. 0. 4. 0. 4. 1.\n",
      " 1. 2. 4. 4. 2. 1. 5. 1. 2. 3. 1. 0. 2. 0. 0. 1. 1. 0. 1. 0. 5. 1. 2. 1.\n",
      " 2. 1. 1. 0. 0. 1. 1. 2. 0. 2. 2. 0. 1. 0. 1. 1. 1. 1. 5. 3. 0. 0. 1. 1.\n",
      " 0. 0. 3. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 2. 0. 0. 1. 0. 0. 1. 2.\n",
      " 1. 0. 2. 1. 0. 0. 1. 0. 6. 4. 0. 0. 1. 1. 1. 0. 2. 1. 2. 1. 2. 1. 0. 1.\n",
      " 0. 1. 2. 0. 3. 2. 1. 2. 1. 2. 3. 0. 4. 2. 4. 2. 2. 3. 3. 1. 2. 0. 1. 1.\n",
      " 0. 3. 3. 1. 2. 0. 1. 2. 0. 2. 4. 0. 1. 1. 1. 4. 0. 0. 2. 0. 1. 4. 2. 2.\n",
      " 1. 0. 1. 1. 1. 0. 2. 1. 2. 0. 1. 1. 0. 0. 1. 1. 5. 2. 2. 2. 0. 1. 3. 0.\n",
      " 1. 2. 3. 3. 2. 1. 5. 1. 3. 2. 1. 2. 1. 0. 3. 0. 2. 3. 1. 0. 3. 0. 3. 1.\n",
      " 3. 1. 2. 3. 1. 0. 1. 1. 0. 2. 3. 1. 3. 3. 0. 0. 2. 0. 2. 1. 0. 1. 3. 3.\n",
      " 1. 2. 1. 3. 4. 4. 0. 4. 1. 1. 1. 2. 3. 1. 2. 2. 2. 2. 1. 0. 0. 0. 5. 5.\n",
      " 0. 4. 0. 0. 1. 1. 3. 1. 3. 0. 4. 0. 0. 1. 5. 0. 2. 3. 5. 0. 2. 0. 0. 0.\n",
      " 0. 0. 5. 2. 3. 0. 3. 1. 3. 2. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 2. 0. 0. 0.\n",
      " 1. 1. 3. 0. 4. 0. 0. 2. 2. 0. 0. 1. 0. 4. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[333])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to Term Frequency - Inverse Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf.transform(X).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append additional categories to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to covert cat descriptors to histogram and append to training data\n",
    "\n",
    "# load cat descriptors \n",
    "cat_desc = np.load('data/20190120-sampledesc-cats.npy')\n",
    "cat_img = np.load('data/20190120-img-cats.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmin(cdist(cat_desc, kmeans_numpy, 'euclidean'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cats = build_code_book(cat_img, y)\n",
    "X_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(X_cats)\n",
    "X_train_cats = tfidf.transform(X_cats).todense()\n",
    "X_train_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack((X_train, X_train_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate targets\n",
    "\n",
    "y_train = np.empty(0)\n",
    "y_train = np.append(y_train, np.ones(500))\n",
    "y_train = np.append(y_train, np.zeros(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acc: 0.646\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Acc:\", (y_train == pred).sum() / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate testing set descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 500 completed\n"
     ]
    }
   ],
   "source": [
    "dog_img, dog_kp, dog_desc = run_extract(dogs[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 500 completed\n"
     ]
    }
   ],
   "source": [
    "cat_img, cat_kp, cat_desc = run_extract(cats[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = np.vstack((dog_desc, cat_desc))\n",
    "img = np.vstack((dog_img, cat_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.argmin(cdist(desc, kmeans_numpy, 'euclidean'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = build_code_book(img, y)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.fit(X_test)\n",
    "X_test = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate targets\n",
    "y_test = np.append(np.ones(500), np.zeros(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.367\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Acc:\", (y_test == y_pred).sum() / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "- Remove redundant codeblocks here\n",
    "- Redo training by running tdidif on entire training set rather separately\n",
    "- Choose better descriptors before clustering\n",
    "- More data\n",
    "- Less clusters\n",
    "    - Read about large vs. small cluster sizes; I think it should be less than 500.\n",
    "- Hyperparameter tuning \n",
    "    - AWS job to run through several K's\n",
    "- Need AWS credits!\n",
    "\n",
    "\n",
    "## Packages Needed:\n",
    "- **pandas** for using scikit\n",
    "- **scikit-learn** for kmeans & SVM\n",
    "\n",
    "## DAN2 Research:\n",
    "- How can we reduce the dictionary?\n",
    "- Should we pre-process images more than grey-scaling? (e.g., HOG, edge detectiony, feature detection)\n",
    "    - explore opencv4 for latest offerings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addtional Info\n",
    "http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
